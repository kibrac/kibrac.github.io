<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>技术 on Kibrac&#39;s Blog</title>
    <link>https://kibrac.github.io/tags/%E6%8A%80%E6%9C%AF/</link>
    <description>Recent content in 技术 on Kibrac&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Aug 2025 00:19:52 +0800</lastBuildDate>
    <atom:link href="https://kibrac.github.io/tags/%E6%8A%80%E6%9C%AF/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>本地部署大模型——Safetensors 格式</title>
      <link>https://kibrac.github.io/posts/%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%A4%A7%E6%A8%A1%E5%9E%8Bsafetensors-%E6%A0%BC%E5%BC%8F/</link>
      <pubDate>Mon, 18 Aug 2025 00:19:52 +0800</pubDate>
      <guid>https://kibrac.github.io/posts/%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%A4%A7%E6%A8%A1%E5%9E%8Bsafetensors-%E6%A0%BC%E5%BC%8F/</guid>
      <description>&lt;p&gt;偶然看到这样一篇公众号文章：&lt;a href=&#34;https://mp.weixin.qq.com/s/SbsYn-DO9UAkyu09C9mwIg&#34;&gt;Baichuan-M2：百川的医疗答卷｜模型解读&lt;/a&gt;，于是一时兴起决定拿来作为自己第一个本地部署大模型的练手项目。&lt;/p&gt;&#xA;&lt;h2 id=&#34;环境和模型来源&#34;&gt;环境和模型来源&lt;/h2&gt;&#xA;&lt;p&gt;本机环境是 4*2080ti 共 88G 显存，关于模型运行的硬件配置要求可以参考 &lt;a href=&#34;https://aibook.ren/archives/llm-deploy-computility-table&#34;&gt;这篇文章&lt;/a&gt; 以及模型自身的参考文档。&lt;/p&gt;&#xA;&lt;p&gt;模型下载链接：&lt;a href=&#34;https://huggingface.co/baichuan-inc/Baichuan-M2-32B&#34;&gt;Baichuan-M2-32B · HuggingFace&lt;/a&gt;，当然也可以选择国内的魔搭社区：&lt;a href=&#34;https://www.modelscope.cn/models/baichuan-inc/Baichuan-M2-32B&#34;&gt;Baichuan-M2-32B · ModelScope&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;p&gt;此外还需要本机配置好 Ollama，尽管 Ollama 现在已经拥有了自己的客户端界面，但你仍然可以选择接入到例如 CherryStudio 中以使用更多功能。关于软件的配置非常简单，这里提供一个参考（这里包括了如何将 Ollama 中的模型导入 CherryStudio 中，虽然笔者觉得后者在使用上已经足够浅显了）：&lt;a href=&#34;https://developer.aliyun.com/article/1669891&#34;&gt;DeepSeek R1+Ollama+Cherry Studio实现本地知识库的搭建-阿里云开发者社区&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;h2 id=&#34;部署过程&#34;&gt;部署过程&lt;/h2&gt;&#xA;&lt;p&gt;由于从模型库下载得到的模型文件为 Safetensors 格式，所以本次部署采用的方法是先使用 &lt;a href=&#34;https://github.com/ggml-org/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; 工具将模型文件转换为 GGUF 格式（事实上 Ollama 可以直接导入 Safetensors 格式的模型文件，但是笔者只是一个什么也不懂的小白，实在是不想去处理五花八门的配置文件，所以选择了这样一个貌似更傻瓜式的处理方式——殊不知为了避开麻烦只会遇到更多麻烦）。&lt;/p&gt;&#xA;&lt;p&gt;如果顺利的话，参考以下教程可以非常简单地完成从模型下载到格式转换最后成功部署，本文对于大多数教程都会呈现的步骤不予重复，只记录个人实践中存在的问题。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_41688410/article/details/145797979&#34;&gt;Ollama部署本地大模型DeepSeek-R1-Distill-Llama-70B-CSDN博客&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;编译问题令人头疼的-cmake&#34;&gt;编译问题：令人头疼的 Cmake&lt;/h2&gt;&#xA;&lt;p&gt;在笔者的实操过程中，最费时间的就是对 llama.cpp 进行编译安装的这一步（时不时弹出的环境和模型文件下载报错也是件糟心的事情，但这通常是网络的问题只能姑且受着，也可以采取换源等方法进行解决这里不做阐述），由于笔者是在 Windows 操作系统上进行部署，并不具备 Linux 的天然优势，所以必须自行配置好 Cmake 环境。&lt;/p&gt;&#xA;&lt;h3 id=&#34;下载安装&#34;&gt;下载安装&lt;/h3&gt;&#xA;&lt;p&gt;比较简易的方法是下载 Visual Studio（注意不是 Visual Studio Code，这两个有本质上的区别），在安装时勾选“C++ 桌面开发”（Desktop development with C++），并确保”Windows 10/11 SDK”被选中（通常默认包含）。它会自动包含 &lt;code&gt;nmake&lt;/code&gt; （Windows 的 make 工具）和其他必要的编译工具。&lt;/p&gt;&#xA;&lt;p&gt;也可以选择自行配置 MinGW（GCC 在 Windows 上的移植版）——对于嫌麻烦的笔者来说选择上一种方法。这里两种方法的教程都附上：&lt;a href=&#34;https://blog.csdn.net/qq_51341501/article/details/148776334&#34;&gt;在 Windows 上安装和编译 llama.cpp-CSDN博客&lt;/a&gt;。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
